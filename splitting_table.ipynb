{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4u81uAGBivp","executionInfo":{"status":"ok","timestamp":1747478597374,"user_tz":-480,"elapsed":36585,"user":{"displayName":"Rowan Delgado Marquez","userId":"05270833907102188737"}},"outputId":"897db64d-e87f-4b36-f873-be00c02693ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","📁 Processing: Brown2019\n","✅ Done: Brown2019 → train: 29, validation: 30, test: 98\n","------------------------------------------------------------\n","📁 Processing: granada\n","✅ Done: granada → train: 111, validation: 111, test: 337\n","------------------------------------------------------------\n","📁 Processing: lynch\n","✅ Done: lynch → train: 15, validation: 16, test: 63\n","------------------------------------------------------------\n","📁 Processing: tamborlane\n","✅ Done: tamborlane → train: 3, validation: 3, test: 20\n","------------------------------------------------------------\n","📁 Processing: wadwa\n","✅ Done: wadwa → train: 15, validation: 16, test: 52\n","------------------------------------------------------------\n"]}],"source":["# Mount Drive, import libraries, read & filter data, split, and save to CSVs\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import os\n","\n","# Step 1: Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Step 2: Dataset names and base folder path\n","dataset_names = ['Brown2019', 'granada', 'lynch', 'tamborlane', 'wadwa']\n","base_path = '/content/drive/MyDrive/Digital Health/INternalAssignment1/TrainValidationTestSplit/'\n","\n","# Step 3: Loop through each dataset\n","for name in dataset_names:\n","    print(f\"📁 Processing: {name}\")\n","\n","    # Full file path\n","    input_file = os.path.join(base_path, f\"{name}_demographic_imputed_labeled.csv\")\n","\n","    # Read CSV\n","    df = pd.read_csv(input_file)\n","\n","    # Filter out classes with < 2 samples\n","    class_counts = df[\"label\"].value_counts()\n","    valid_classes = class_counts[class_counts >= 2].index\n","    df_filtered = df[df[\"label\"].isin(valid_classes)]\n","\n","    # First split: 60% test, 40% remaining (train+val)\n","    train_val, test = train_test_split(\n","        df_filtered,\n","        test_size=0.6,\n","        random_state=42,\n","        stratify=df_filtered[\"label\"]\n","    )\n","\n","    # Re-filter train_val after split to ensure all labels still have ≥2 samples\n","    class_counts = train_val[\"label\"].value_counts()\n","    valid_classes = class_counts[class_counts >= 2].index\n","    train_val_filtered = train_val[train_val[\"label\"].isin(valid_classes)]\n","\n","    # Second split: 50/50 of 40% remaining → 20% train, 20% validation\n","    train, validation = train_test_split(\n","        train_val_filtered,\n","        test_size=0.5,\n","        random_state=42,\n","        stratify=train_val_filtered[\"label\"]\n","    )\n","\n","    # Save all splits to CSV\n","    train.to_csv(os.path.join(base_path, f\"{name.lower()}_train.csv\"), index=False)\n","    validation.to_csv(os.path.join(base_path, f\"{name.lower()}_validation.csv\"), index=False)\n","    test.to_csv(os.path.join(base_path, f\"{name.lower()}_test.csv\"), index=False)\n","\n","    print(f\"✅ Done: {name} → train: {len(train)}, validation: {len(validation)}, test: {len(test)}\")\n","    print(\"-\" * 60)\n"]}]}